{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOw2BUYkTJJFfbJzI3HvFmT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Polyakri/projects/blob/main/MachineLearningProjects/RockPaperScissors_Game/4_CNN_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import the necessary Libraries"
      ],
      "metadata": {
        "id": "S5zPAnB8_fOr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR7y1YzqT-cp",
        "outputId": "3cf4f702-46ff-4415-8983-5ac38211c9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from itertools import product\n",
        "from os.path import exists\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare the data for the CNN"
      ],
      "metadata": {
        "id": "BpXFnggq_vcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = np.load('drive/MyDrive/MLFinalEx/NoPCA/x_data_full_flipped.npy')\n",
        "y_data = np.load('drive/MyDrive/MLFinalEx/NoPCA/y_data_full_flipped.npy')\n",
        "\n",
        "x_data = np.reshape (x_data, (x_data.shape[0],200,300,1))"
      ],
      "metadata": {
        "id": "q6Ypfca4UIQx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resize the data into smaller pictures, it took way too much time to proceed with the 200x300 images."
      ],
      "metadata": {
        "id": "gy9RvRb4_3V7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_size = (75, 50)\n",
        "\n",
        "# Create an empty array to store resized images\n",
        "resized_images = np.empty((x_data.shape[0], target_size[1], target_size[0]))\n",
        "\n",
        "# Loop through each image and resize\n",
        "for i in range(x_data.shape[0]):\n",
        "    img = x_data[i]\n",
        "\n",
        "    # Resize the image using OpenCV\n",
        "    resized_img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Store the resized image in the new array\n",
        "    resized_images[i] = resized_img\n",
        "\n",
        "print(\"Resizing complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2niYGmedV7x",
        "outputId": "9bd6991a-4eea-47ff-f841-dfb7696da2fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resizing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = resized_images"
      ],
      "metadata": {
        "id": "p3aVCkPhdjiN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into training and test set"
      ],
      "metadata": {
        "id": "0Pu1lbAW_0UV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
        ")\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uZ3x3BFVn1k",
        "outputId": "cbc4bcff-2eb3-4bd0-9785-547e8f8f1230"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (3500, 50, 75)\n",
            "x_test shape: (876, 50, 75)\n",
            "y_train shape: (3500,)\n",
            "y_test shape: (876,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the Convolutional Neural Networks"
      ],
      "metadata": {
        "id": "bXS3mhMrAEnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training all these neural networks will indeed take a lot time so when I found 99.5 % accuracy I stopped"
      ],
      "metadata": {
        "id": "U2WOXHnJAJ2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = ['accuracy']\n",
        "loss = 'binary_crossentropy'\n",
        "num_classes =3\n",
        "y_test = tf.one_hot(y_test, depth=num_classes)\n",
        "y_train = tf.one_hot(y_train, depth=num_classes)\n",
        "y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyMwkzm7aFF3",
        "outputId": "c4723a05-2181-41fc-cc5a-9197be9310eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([3500, 3]), TensorShape([876, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_conv_layers = [1,2,3]\n",
        "num_filters = [32, 64]\n",
        "kernel_size = [(5, 5), (3, 3)]\n",
        "strides = (1, 1)\n",
        "mlp_num_hidden_layers = [1, 2, 3]\n",
        "mlp_units_per_layer = [32, 64, 128, 256, 512]\n",
        "activation = ['relu', 'sigmoid', 'tanh']\n",
        "dropout_rate = [0.4]\n",
        "batch_size = [32]\n",
        "epochs = [50]\n",
        "learning_rate = [0.001, 0.01, 0.1]\n",
        "output_activations = ['softmax', 'sigmoid']\n",
        "\n",
        "#Put the correct file location\n",
        "csv_filename = 'drive/MyDrive/MLFinalEx/CNN_full_flipped_results_temp2.csv'\n",
        "\n",
        "if exists(csv_filename):\n",
        "    results_df = pd.read_csv(csv_filename)\n",
        "else:\n",
        "    results_df = pd.DataFrame(columns=['Num_Conv_Layers', 'Num_Filters', 'Kernel_Size', 'MLP_Num_Hidden_Layers',\n",
        "                                       'MLP_Units_Per_Layer', 'Activation', 'Dropout_Rate', 'Batch_Size', 'Epochs',\n",
        "                                       'Learning_Rate', 'Output_Activation', 'Accuracy'])\n",
        "\n",
        "# Generate all combinations of hyperparameters\n",
        "param_combinations = product(\n",
        "    num_conv_layers,\n",
        "    num_filters,\n",
        "    kernel_size,\n",
        "    mlp_num_hidden_layers,\n",
        "    mlp_units_per_layer,\n",
        "    activation,\n",
        "    dropout_rate,\n",
        "    batch_size,\n",
        "    epochs,\n",
        "    learning_rate,\n",
        "    output_activations\n",
        ")\n",
        "\n",
        "for params in param_combinations:\n",
        "    num_conv, filters, k_size, mlp_layers, mlp_units, act, dropout, b_size, epoch, lr, output_activation = params\n",
        "\n",
        "    # Check if the combination already exists in the DataFrame\n",
        "    if results_df[\n",
        "        (results_df['Num_Conv_Layers'] == num_conv) &\n",
        "        (results_df['Num_Filters'] == filters) &\n",
        "        (results_df['Kernel_Size'] == str(k_size)) &  # Convert tuple to string for comparison\n",
        "        (results_df['MLP_Num_Hidden_Layers'] == mlp_layers) &\n",
        "        (results_df['MLP_Units_Per_Layer'] == mlp_units) &\n",
        "        (results_df['Activation'] == act) &\n",
        "        (results_df['Dropout_Rate'] == dropout) &\n",
        "        (results_df['Batch_Size'] == b_size) &\n",
        "        (results_df['Epochs'] == epoch) &\n",
        "        (results_df['Learning_Rate'] == lr) &\n",
        "        (results_df['Output_Activation'] == output_activation)\n",
        "    ].shape[0] > 0:\n",
        "        print(f\"Skipping existing combination: {params}\")\n",
        "        continue\n",
        "\n",
        "    cnn = tf.keras.Sequential(name='my-cnn')\n",
        "    cnn.add(tf.keras.layers.Input((50, 75, 1), name='input_layer'))\n",
        "\n",
        "    for i in range(num_conv):\n",
        "        cnn.add(tf.keras.layers.Conv2D(filters=filters, kernel_size=k_size, strides=strides, activation=act,\n",
        "                                       name=f'conv_layer{i + 1}'))\n",
        "        cnn.add(tf.keras.layers.Dropout(rate=dropout, name=f'dropout_{i + 1}'))\n",
        "        cnn.add(tf.keras.layers.MaxPooling2D(name=f'maxpool_{i + 1}'))\n",
        "\n",
        "    # Flattening\n",
        "    cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "    # MLP\n",
        "    for i in range(mlp_layers):\n",
        "        cnn.add(tf.keras.layers.Dense(units=mlp_units, activation=act, name=f'hidden_layer_{i + 1}'))\n",
        "\n",
        "    cnn.add(tf.keras.layers.Dense(units=num_classes, activation=output_activation, name='output_layer'))\n",
        "\n",
        "    cnn.summary ()\n",
        "\n",
        "    cnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    # Assuming x_train, y_train, x_test, y_test are defined\n",
        "    history = cnn.fit(x_train, y_train, batch_size=b_size, epochs=epoch, validation_data=(x_test, y_test), verbose=1)\n",
        "\n",
        "    # Get the final accuracy from the training history\n",
        "    accuracy = history.history['val_accuracy'][-1]\n",
        "\n",
        "    # Append results to the DataFrame\n",
        "    results_df = results_df.append({\n",
        "        'Num_Conv_Layers': num_conv,\n",
        "        'Num_Filters': filters,\n",
        "        'Kernel_Size': str(k_size),  # Convert tuple to string for storage\n",
        "        'MLP_Num_Hidden_Layers': mlp_layers,\n",
        "        'MLP_Units_Per_Layer': mlp_units,\n",
        "        'Activation': act,\n",
        "        'Dropout_Rate': dropout,\n",
        "        'Batch_Size': b_size,\n",
        "        'Epochs': epoch,\n",
        "        'Learning_Rate': lr,\n",
        "        'Output_Activation': output_activation,\n",
        "        'Accuracy': accuracy\n",
        "    }, ignore_index=True)\n",
        "\n",
        "    # Save the results after every iteration\n",
        "    results_df.to_csv(csv_filename, index=False)\n",
        "\n",
        "    # Display the results\n",
        "    print(f\"Hyperparameters: {params}, Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "bi9KZ0SGVA-F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}